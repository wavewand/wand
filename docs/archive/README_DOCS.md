# MCP Python - Multi-Framework AI Platform

A comprehensive, production-ready AI platform that provides unified access to multiple AI frameworks and services through a single, powerful API.

## Overview

MCP Python is an advanced multi-framework AI platform designed to integrate with the most popular AI frameworks and APIs, providing a unified interface for AI operations. The platform supports intelligent query routing, advanced analytics, document processing, agent management, and stateful workflow execution.

## Supported Frameworks

### Core AI Frameworks
- **Haystack** - Retrieval-augmented generation and document processing
- **LlamaIndex** - Document indexing and intelligent querying
- **LangChain** - Chains, agents, and conversational AI
- **LangGraph** - Stateful workflows and multi-step reasoning

### Direct API Integrations
- **OpenAI** - GPT models, embeddings, and completions
- **Transformers (Hugging Face)** - Local model inference and fine-tuning
- **Anthropic Claude** - Advanced reasoning and conversational AI
- **Cohere** - Text generation, embeddings, and reranking

### Custom Framework Support
- **Custom Framework Adapter** - Integrate any AI API through configuration
- **Framework Registry** - Manage and discover custom adapters

## Key Features

### ðŸš€ Multi-Framework Query Processing
- Intelligent framework selection based on query type
- Unified query interface across all frameworks
- Automatic fallback and error handling
- Batch processing capabilities

### ðŸ” Advanced Search & Analytics
- Full-text search across all entities
- 13+ search operators with complex filtering
- Real-time analytics and performance monitoring
- Usage statistics and trend analysis

### ðŸ“Š Production-Ready Infrastructure
- Comprehensive error handling and resilience patterns
- Circuit breakers and retry mechanisms
- Rate limiting and usage tracking
- Structured logging and observability

### ðŸ” Security & Authentication
- JWT-based authentication
- API key management
- Role-based access control (RBAC)
- Input validation and sanitization

### ðŸ—„ï¸ Data Management
- Database integration with SQLAlchemy
- Document storage and retrieval
- Vector embeddings and similarity search
- Migration system and connection pooling

### ðŸ¤– Agent & Workflow Management
- Multi-framework agent orchestration
- Stateful workflow execution with LangGraph
- Conversation management and memory
- Tool integration and custom functions

### ðŸ”— Claude Code Integration (MCP)
- **Model Context Protocol (MCP)** support for Claude Code
- Direct integration with Claude Code CLI and Desktop
- Real-time system monitoring via MCP tools
- Agent and task management through Claude interface

### ðŸ“ˆ Monitoring & Analytics
- Real-time performance metrics
- Framework-specific analytics
- Resource usage monitoring
- Custom reporting and dashboards

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API      â”‚    â”‚   WebSocket     â”‚    â”‚   gRPC          â”‚
â”‚   (FastAPI)     â”‚    â”‚   Events        â”‚    â”‚   Services      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚           Framework Registry                â”‚
          â”‚     (Plugin Architecture)                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               â”‚               â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Haystackâ”‚    â”‚LlamaIndexâ”‚    â”‚ Future   â”‚
â”‚ Agent  â”‚    â”‚  Agent   â”‚    â”‚Framework â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“¦ Installation

### Prerequisites
- Python 3.8+
- Docker and Docker Compose (for containerized deployment)
- Redis (optional, for distributed caching)

### Quick Start

1. **Clone the repository**:
```bash
git clone https://github.com/your-org/mcp-python.git
cd mcp-python
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Set up environment variables**:
```bash
cp .env.example .env
# Edit .env with your configuration
```

4. **Start the services**:
```bash
# Option 1: Direct Python execution
python api/server.py

# Option 2: Docker Compose
docker-compose up -d
```

## ðŸš€ Quick Usage Examples

### Basic RAG Query
```python
import requests

# Execute a RAG query using Haystack
response = requests.post("http://localhost:8000/api/v1/rag", json={
    "query": "What is artificial intelligence?",
    "framework": "haystack",
    "pipeline_id": "default_rag",
    "max_tokens": 500
})

result = response.json()
print(result["answer"])
```

### Document Ingestion
```python
# Ingest a document for search and RAG
response = requests.post("http://localhost:8000/api/v1/documents", json={
    "filename": "ai_guide.txt",
    "content": "Artificial Intelligence is...",
    "framework": "haystack",
    "metadata": {"category": "education"}
})
```

### Batch Processing
```python
# Process multiple queries in batch
response = requests.post("http://localhost:8000/api/v1/batch/rag", json={
    "queries": [
        "What is machine learning?",
        "Explain neural networks",
        "What are transformers?"
    ],
    "framework": "llamaindex",
    "batch_size": 3
})
```

### Framework Benchmarking
```python
# Compare performance across frameworks
response = requests.post("http://localhost:8000/api/v1/benchmark/performance", json={
    "frameworks": ["haystack", "llamaindex"],
    "iterations": 10
})

benchmark = response.json()["benchmark"]
print(f"Winner: {benchmark['winner']}")
```

## ðŸ”— Claude Code Integration (MCP)

### Quick Setup
1. **Deploy the MCP Platform**:
```bash
cd mcp-automation
./scripts/docker-build.sh && ./scripts/deploy-docker.sh
```

2. **Add MCP Server to Claude Code**:
```bash
claude mcp add -t http mcp-automation-api http://localhost:8000/mcp
```

3. **Test the Integration**:
```bash
claude --print "Use the get_system_status tool" --allowedTools "mcp__mcp-automation-api__*"
```

### Available MCP Tools
- **get_system_status** - Get current system status and metrics
- **list_agents** - List all available agents with optional filtering
- **create_task** - Create new tasks in the automation platform

### Usage Examples
```bash
# Check system status via Claude Code
claude --print "Check the automation platform status" --allowedTools "mcp__mcp-automation-api__get_system_status"

# Create a task via Claude Code
claude --print "Create a task for data processing" --allowedTools "mcp__mcp-automation-api__create_task"

# List agents via Claude Code
claude --print "Show me all available agents" --allowedTools "mcp__mcp-automation-api__list_agents"
```

ðŸ“š **Complete Documentation**: See [docs/MCP_CLAUDE_CODE_INTEGRATION.md](./docs/MCP_CLAUDE_CODE_INTEGRATION.md)

## ðŸ”Œ API Reference

### Core Endpoints

| Endpoint | Method | Description |
|----------|---------|-------------|
| `/api/v1/rag` | POST | Execute RAG query |
| `/api/v1/search` | POST | Search documents |
| `/api/v1/documents` | POST | Ingest document |
| `/api/v1/summarize` | POST | Summarize text |
| `/api/v1/ai/frameworks` | GET | List available frameworks |

### Batch Processing

| Endpoint | Method | Description |
|----------|---------|-------------|
| `/api/v1/batch/rag` | POST | Batch RAG queries |
| `/api/v1/batch/documents` | POST | Batch document ingestion |
| `/api/v1/batch/search` | POST | Batch search queries |
| `/api/v1/batch/status/{id}` | GET | Get batch status |

### Benchmarking

| Endpoint | Method | Description |
|----------|---------|-------------|
| `/api/v1/benchmark/performance` | POST | Performance benchmark |
| `/api/v1/benchmark/throughput` | POST | Throughput benchmark |
| `/api/v1/benchmark/resources` | POST | Resource usage benchmark |
| `/api/v1/benchmark/results/{id}` | GET | Get benchmark results |

### Caching

| Endpoint | Method | Description |
|----------|---------|-------------|
| `/api/v1/cache/stats` | GET | Cache statistics |
| `/api/v1/cache/entries` | GET | Cache entries |
| `/api/v1/cache/invalidate` | POST | Invalidate cache |
| `/api/v1/cache/clear` | POST | Clear all cache |

## ðŸ“Š Monitoring & Observability

### Health Check
```bash
curl http://localhost:8000/health
```

### Metrics Collection
```bash
# Get framework performance metrics
curl http://localhost:8000/api/v1/monitoring/metrics

# Get cache statistics
curl http://localhost:8000/api/v1/cache/stats

# Get WebSocket connection stats
curl http://localhost:8000/api/v1/websocket/stats
```

### Real-Time Events via WebSocket
```javascript
const ws = new WebSocket('ws://localhost:8000/ws');

ws.onmessage = function(event) {
    const data = JSON.parse(event.data);
    console.log('Event:', data.type, data.message);
};

// Set event filters
ws.send(JSON.stringify({
    action: "update_filters",
    event_types: ["framework_operation", "batch_operation"],
    priority_filter: "normal"
}));
```

## ðŸš€ Advanced Features

### Custom Framework Integration

```python
from frameworks.base_framework import BaseFramework

class MyCustomFramework(BaseFramework):
    def __init__(self):
        super().__init__("mycustom", ["rag_query", "document_search"])

    async def execute_rag_query(self, query: str, **kwargs):
        # Implement your RAG logic
        return {"answer": "...", "success": True}

    async def search_documents(self, query: str, **kwargs):
        # Implement your search logic
        return {"documents": [...], "success": True}

# Register the framework
from ai_framework_registry import ai_framework_registry
ai_framework_registry.register_framework("mycustom", MyCustomFramework())
```

### Custom Event Listeners

```python
from websocket.event_manager import websocket_event_manager, EventType

async def my_event_handler(event):
    if event.data.get("framework") == "haystack":
        print(f"Haystack operation: {event.message}")

websocket_event_manager.add_event_listener(
    EventType.FRAMEWORK_OPERATION,
    my_event_handler
)
```

### Response Caching Decorator

```python
from caching.response_cache import cache_response

@cache_response("haystack", "rag_query")
async def my_rag_function(query: str):
    # This function's results will be automatically cached
    return await perform_rag_query(query)
```

## ðŸ§ª Testing

### Run Tests
```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_frameworks/
pytest tests/test_api/
pytest tests/test_caching/

# Run with coverage
pytest --cov=. --cov-report=html
```

### Integration Tests
```bash
# Test complete system
python test_complete_system.py

# Test framework compatibility
python test_multi_framework_api.py
```

## ðŸ³ Docker Deployment

### Docker Compose
```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Environment Variables
```env
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=false

# Framework Configuration
HAYSTACK_API_KEY=your_api_key
LLAMAINDEX_API_KEY=your_api_key

# Caching Configuration
CACHE_MAX_SIZE=1000
CACHE_DEFAULT_TTL=3600

# Monitoring Configuration
ENABLE_MONITORING=true
METRICS_EXPORT_FORMAT=json
```

## ðŸ“ˆ Performance

### Benchmarks
- **RAG Queries**: ~200ms average response time
- **Document Search**: ~150ms average response time
- **Batch Processing**: 10x faster than individual requests
- **Cache Hit Rate**: >85% for repeated queries
- **WebSocket Events**: <10ms latency

### Scalability
- Supports 1000+ concurrent connections
- Horizontal scaling via Docker/Kubernetes
- Distributed caching with Redis
- Load balancing compatible

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ðŸ“„ License

This project is proprietary software. All rights reserved.
